{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import transaction data and aggregate it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This workflow will demonstrate how to connect SQLite database with python to import and aggregate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "Create scripts that will process the CSV files, insert data into table and use that data in next step for performing various aggregations on it. The columns in the CSV files are as follow:\n",
    "\n",
    "“transactionId”, // a unique transaction identifier <br />\n",
    "“user”, // a unique user identifier <br />\n",
    "“datetime”, // a timestamp in ms (javascript format) <br />\n",
    "“operation”, // a description of the transaction being charged <br />\n",
    "“quantity”, <br />\n",
    "“unitPrice” // the revenue of the transaction is quantity * unit_price <br />\n",
    "We have use SQLite as a databse so the test is self-contained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected Result\n",
    "\n",
    "### Part 1: Create an import script\n",
    "\n",
    "In the first part, we will create a script that imports the transactions of the CSV files into an SQL table.\n",
    "\n",
    "- The script should accept the CSV filename.\n",
    "- It should put the information into the DB, in a table that collects all transactions.\n",
    "- All data from CSVs imported by the script will be placed in this same table.\n",
    "- It should handle duplicate rows (i.e. same CSV imported twice by mistake or update existing data by processing a new file).\n",
    "\n",
    "### Part 2: Create an aggregation script\n",
    "\n",
    "In the second part, we will create a program that reads from the SQL table from the previous part and aggregates data into two new DB tables that allow to get the following information:\n",
    "\n",
    "- Aggregated by user / day: Number of operations and revenue per User per Day\n",
    "- Aggregated by user / hour: Number of operations and revenue per User per Hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "###### First, Import SQLite3 as a database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from csv import reader\n",
    "from csv import DictReader\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Second, create a Database object and connect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = 'sample.db'\n",
    "connection = sqlite3.connect(database)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cur = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Read CSV file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('transactions_2020-08-01.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get the details about the data stored in CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       transactionId     user       datetime    operation  quantity  unitPrice\n",
      "0              10000  user_23  1596265204486  OPERATION_3         6      58.77\n",
      "1              10001  user_19  1596265208999  OPERATION_3         7      65.35\n",
      "2              10002  user_69  1596265211607  OPERATION_6         1      36.57\n",
      "3              10003  user_65  1596265215179  OPERATION_5         8      98.37\n",
      "4              10004  user_21  1596265223091  OPERATION_4         2      74.43\n",
      "...              ...      ...            ...          ...       ...        ...\n",
      "19995          29995  user_97  1596351585521  OPERATION_2         5      84.54\n",
      "19996          29996  user_13  1596351592813  OPERATION_1         1      37.30\n",
      "19997          29997  user_74  1596351594605  OPERATION_7         1      21.99\n",
      "19998          29998  user_68  1596351595906  OPERATION_2         6      48.98\n",
      "19999          29999   user_2  1596351599448  OPERATION_1         3      69.57\n",
      "\n",
      "[20000 rows x 6 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   transactionId  20000 non-null  int64  \n",
      " 1   user           20000 non-null  object \n",
      " 2   datetime       20000 non-null  int64  \n",
      " 3   operation      20000 non-null  object \n",
      " 4   quantity       20000 non-null  int64  \n",
      " 5   unitPrice      20000 non-null  float64\n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 781.3+ KB\n"
     ]
    }
   ],
   "source": [
    "print(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create table named 'tran' to store the records fetched from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0xe87af20>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur = connection.cursor()\n",
    "sql = '''\n",
    "CREATE TABLE tran(\n",
    "transactionId INT, \n",
    "user TEXT, \n",
    "datetime NUMERIC, \n",
    "operation TEXT, \n",
    "quantity INT, \n",
    "unitPrice NUMERIC\n",
    ")'''\n",
    "cur.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Open the CSV file, read the data and insert these records into tran table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records Inserted successfully........\n"
     ]
    }
   ],
   "source": [
    "with open('transactions_2020-08-02.csv', 'r') as read_obj:\n",
    "    csv_dict_reader = DictReader(read_obj)\n",
    "    for row in csv_dict_reader:\n",
    "        s = row['datetime']\n",
    "        date = int(s) / 1000.0\n",
    "        new_date = datetime.datetime.fromtimestamp(date).strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "        #print(row['transactionId'], row['user'],row['datetime'],row['operation'],row['quantity'],row['unitPrice'],new_date)\n",
    "        cur.execute('''INSERT INTO tran values (?, ?, ?, ?, ?, ?) ''', ( row['transactionId'], row['user'],row['datetime'],row['operation'],row['quantity'],row['unitPrice']))\n",
    "    connection.commit()\n",
    "    print(\"Records Inserted successfully........\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create new table named 'Agg_User_Per_Day' to store 'Number of operations and revenue per User per Day'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully........\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE TABLE Agg_User_Per_Day(\n",
    "user TEXT,\n",
    "No_of_Operations INTEGER,\n",
    "Revenue NUMERIC\n",
    ")'''\n",
    "cur.execute(sql)\n",
    "print(\"Table created successfully........\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Aggregate the data fetched from tran table and insert it into new table.\n",
    "This will perform following:\n",
    "- Aggregated by user / day: Number of operations and revenue per User per Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('user_0', 165, 302.1), ('user_1', 193, 10.83), ('user_10', 162, 244.89999999999998), ('user_11', 159, 42.51), ('user_12', 173, 84.87), ('user_13', 185, 61.67), ('user_14', 171, 167.68), ('user_15', 164, 498.26000000000005), ('user_16', 159, 339.04), ('user_17', 171, 22.05), ('user_18', 165, 751.44), ('user_19', 195, 135.75), ('user_2', 157, 174.4), ('user_20', 163, 264), ('user_21', 198, 61.63), ('user_22', 191, 29.73), ('user_23', 176, 686.08), ('user_24', 171, 407.65), ('user_25', 195, 585.4), ('user_26', 179, 674.56), ('user_27', 180, 181.35), ('user_28', 160, 52.0), ('user_29', 186, 75.24), ('user_3', 201, 873.36), ('user_30', 162, 279.02), ('user_31', 174, 55.62), ('user_32', 163, 754.0), ('user_33', 180, 628.92), ('user_34', 172, 223.84), ('user_35', 193, 6.65), ('user_36', 176, 257.46), ('user_37', 165, 250.52), ('user_38', 183, 390.32), ('user_39', 207, 269.4), ('user_4', 213, 317.55), ('user_40', 178, 500.64), ('user_41', 187, 119.72), ('user_42', 180, 16.56), ('user_43', 164, 43.37), ('user_44', 193, 79.0), ('user_45', 196, 136.32), ('user_46', 187, 51.849999999999994), ('user_47', 182, 63.87), ('user_48', 169, 185.64), ('user_49', 191, 697.9499999999999), ('user_5', 166, 383.52), ('user_50', 184, 280.49), ('user_51', 180, 105.56), ('user_52', 166, 20.88), ('user_53', 183, 1.7999999999999998), ('user_54', 155, 523.04), ('user_55', 181, 135.2), ('user_56', 182, 466.48), ('user_57', 154, 287.34000000000003), ('user_58', 173, 166.29999999999998), ('user_59', 187, 37.26), ('user_6', 170, 83.62), ('user_60', 148, 89.06), ('user_61', 150, 86.5), ('user_62', 196, 500.16), ('user_63', 163, 327.32), ('user_64', 198, 68.48), ('user_65', 161, 230), ('user_66', 187, 554.0500000000001), ('user_67', 185, 193.44), ('user_68', 180, 34.32), ('user_69', 187, 380.68), ('user_7', 183, 1.64), ('user_70', 166, 387.54), ('user_71', 164, 462.04999999999995), ('user_72', 187, 702), ('user_73', 187, 184.5), ('user_74', 136, 162.72), ('user_75', 178, 560.21), ('user_76', 161, 322.6), ('user_77', 186, 522.7), ('user_78', 180, 74.97), ('user_79', 175, 526.96), ('user_8', 171, 350.52), ('user_80', 193, 57.36), ('user_81', 183, 237.69), ('user_82', 189, 195.85000000000002), ('user_83', 185, 63.0), ('user_84', 193, 450.6), ('user_85', 179, 186.4), ('user_86', 196, 542.97), ('user_87', 152, 219.08), ('user_88', 180, 751.04), ('user_89', 163, 4.98), ('user_9', 168, 370.08), ('user_90', 188, 64.22), ('user_91', 163, 417.83), ('user_92', 181, 115.92), ('user_93', 162, 102.95), ('user_94', 176, 115.2), ('user_95', 168, 123.68), ('user_96', 181, 286.78999999999996), ('user_97', 177, 675.85), ('user_98', 176, 93.24), ('user_99', 176, 5.75)]\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "           SELECT user,\n",
    "                  count(operation),\n",
    "                  quantity*unitPrice as Revenue \n",
    "           FROM  tran \n",
    "           WHERE strftime('%Y-%m-%d', datetime / 1000, 'unixepoch') = '2020-08-02' \n",
    "           GROUP BY user \n",
    "           '''\n",
    "\n",
    "cur.execute(query)\n",
    "results = cur.fetchall()\n",
    "print(results)\n",
    "\n",
    "cur.executemany('INSERT INTO Agg_User_Per_Day VALUES (?,?,?);',results)\n",
    "# Commit your changes in the database\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Fetch the aggregated data from Agg_User_Per_Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('user_0', 165, 302.1), ('user_1', 193, 10.83), ('user_10', 162, 244.89999999999998), ('user_11', 159, 42.51), ('user_12', 173, 84.87), ('user_13', 185, 61.67), ('user_14', 171, 167.68), ('user_15', 164, 498.26000000000005), ('user_16', 159, 339.04), ('user_17', 171, 22.05), ('user_18', 165, 751.44), ('user_19', 195, 135.75), ('user_2', 157, 174.4), ('user_20', 163, 264), ('user_21', 198, 61.63), ('user_22', 191, 29.73), ('user_23', 176, 686.08), ('user_24', 171, 407.65), ('user_25', 195, 585.4), ('user_26', 179, 674.56), ('user_27', 180, 181.35), ('user_28', 160, 52), ('user_29', 186, 75.24), ('user_3', 201, 873.36), ('user_30', 162, 279.02), ('user_31', 174, 55.62), ('user_32', 163, 754), ('user_33', 180, 628.92), ('user_34', 172, 223.84), ('user_35', 193, 6.65), ('user_36', 176, 257.46), ('user_37', 165, 250.52), ('user_38', 183, 390.32), ('user_39', 207, 269.4), ('user_4', 213, 317.55), ('user_40', 178, 500.64), ('user_41', 187, 119.72), ('user_42', 180, 16.56), ('user_43', 164, 43.37), ('user_44', 193, 79), ('user_45', 196, 136.32), ('user_46', 187, 51.849999999999994), ('user_47', 182, 63.87), ('user_48', 169, 185.64), ('user_49', 191, 697.9499999999999), ('user_5', 166, 383.52), ('user_50', 184, 280.49), ('user_51', 180, 105.56), ('user_52', 166, 20.88), ('user_53', 183, 1.7999999999999998), ('user_54', 155, 523.04), ('user_55', 181, 135.2), ('user_56', 182, 466.48), ('user_57', 154, 287.34000000000003), ('user_58', 173, 166.29999999999998), ('user_59', 187, 37.26), ('user_6', 170, 83.62), ('user_60', 148, 89.06), ('user_61', 150, 86.5), ('user_62', 196, 500.16), ('user_63', 163, 327.32), ('user_64', 198, 68.48), ('user_65', 161, 230), ('user_66', 187, 554.0500000000001), ('user_67', 185, 193.44), ('user_68', 180, 34.32), ('user_69', 187, 380.68), ('user_7', 183, 1.64), ('user_70', 166, 387.54), ('user_71', 164, 462.04999999999995), ('user_72', 187, 702), ('user_73', 187, 184.5), ('user_74', 136, 162.72), ('user_75', 178, 560.21), ('user_76', 161, 322.6), ('user_77', 186, 522.7), ('user_78', 180, 74.97), ('user_79', 175, 526.96), ('user_8', 171, 350.52), ('user_80', 193, 57.36), ('user_81', 183, 237.69), ('user_82', 189, 195.85000000000002), ('user_83', 185, 63), ('user_84', 193, 450.6), ('user_85', 179, 186.4), ('user_86', 196, 542.97), ('user_87', 152, 219.08), ('user_88', 180, 751.04), ('user_89', 163, 4.98), ('user_9', 168, 370.08), ('user_90', 188, 64.22), ('user_91', 163, 417.83), ('user_92', 181, 115.92), ('user_93', 162, 102.95), ('user_94', 176, 115.2), ('user_95', 168, 123.68), ('user_96', 181, 286.78999999999996), ('user_97', 177, 675.85), ('user_98', 176, 93.24), ('user_99', 176, 5.75)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"SELECT * FROM Agg_User_Per_Day \")\n",
    "results = cur.fetchall()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create new table named 'Agg_User_Per_Hour' to store 'Number of operations and revenue per User per Hour'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0xe87af20>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE TABLE Agg_User_Per_Hour(\n",
    "Hour TEXT,\n",
    "user TEXT,\n",
    "No_of_Operations INTEGER,\n",
    "Revenue NUMERIC\n",
    ")'''\n",
    "cur.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Aggregate the data fetched from tran table and insert it into new table. This will perform following:\n",
    "- Aggregated by user / hour: Number of operations and revenue per User per Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2020-08-02 07:00:00', 'user_0', 15, 302.1), ('2020-08-02 07:00:00', 'user_1', 13, 10.83), ('2020-08-02 07:00:00', 'user_10', 10, 244.89999999999998), ('2020-08-02 07:00:00', 'user_11', 11, 42.51), ('2020-08-02 07:00:00', 'user_12', 9, 84.87), ('2020-08-02 07:00:00', 'user_13', 12, 61.67), ('2020-08-02 07:00:00', 'user_14', 9, 167.68), ('2020-08-02 07:00:00', 'user_15', 10, 498.26000000000005), ('2020-08-02 07:00:00', 'user_16', 12, 339.04), ('2020-08-02 07:00:00', 'user_17', 8, 22.05), ('2020-08-02 07:00:00', 'user_18', 8, 751.44), ('2020-08-02 07:00:00', 'user_19', 11, 135.75), ('2020-08-02 07:00:00', 'user_2', 9, 174.4), ('2020-08-02 07:00:00', 'user_20', 13, 264), ('2020-08-02 07:00:00', 'user_21', 9, 61.63), ('2020-08-02 07:00:00', 'user_22', 9, 29.73), ('2020-08-02 07:00:00', 'user_23', 12, 686.08), ('2020-08-02 07:00:00', 'user_24', 15, 407.65), ('2020-08-02 07:00:00', 'user_25', 10, 585.4), ('2020-08-02 07:00:00', 'user_26', 13, 674.56), ('2020-08-02 07:00:00', 'user_27', 8, 181.35), ('2020-08-02 07:00:00', 'user_28', 7, 52.0), ('2020-08-02 07:00:00', 'user_29', 10, 75.24), ('2020-08-02 07:00:00', 'user_3', 10, 873.36), ('2020-08-02 07:00:00', 'user_30', 7, 279.02), ('2020-08-02 07:00:00', 'user_31', 11, 55.62), ('2020-08-02 07:00:00', 'user_32', 4, 754.0), ('2020-08-02 07:00:00', 'user_33', 17, 628.92), ('2020-08-02 07:00:00', 'user_34', 18, 223.84), ('2020-08-02 07:00:00', 'user_35', 14, 6.65), ('2020-08-02 07:00:00', 'user_36', 11, 257.46), ('2020-08-02 07:00:00', 'user_37', 10, 250.52), ('2020-08-02 07:00:00', 'user_38', 11, 390.32), ('2020-08-02 07:00:00', 'user_39', 9, 269.4), ('2020-08-02 07:00:00', 'user_4', 16, 317.55), ('2020-08-02 07:00:00', 'user_40', 6, 500.64), ('2020-08-02 07:00:00', 'user_41', 11, 119.72), ('2020-08-02 07:00:00', 'user_42', 8, 16.56), ('2020-08-02 07:00:00', 'user_43', 10, 43.37), ('2020-08-02 07:00:00', 'user_44', 14, 79.0), ('2020-08-02 07:00:00', 'user_45', 11, 136.32), ('2020-08-02 07:00:00', 'user_46', 7, 51.849999999999994), ('2020-08-02 07:00:00', 'user_47', 7, 63.87), ('2020-08-02 07:00:00', 'user_48', 6, 185.64), ('2020-08-02 07:00:00', 'user_49', 11, 697.9499999999999), ('2020-08-02 07:00:00', 'user_5', 8, 383.52), ('2020-08-02 07:00:00', 'user_50', 12, 280.49), ('2020-08-02 07:00:00', 'user_51', 11, 105.56), ('2020-08-02 07:00:00', 'user_52', 10, 20.88), ('2020-08-02 07:00:00', 'user_53', 9, 1.7999999999999998)]\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "    SELECT  strftime('%Y-%m-%d %H:00:00', datetime / 1000, 'unixepoch') as datetime,\n",
    "            user,\n",
    "            count(operation) as count,\n",
    "            quantity*unitPrice as Revenue\n",
    "    FROM  tran\n",
    "    WHERE strftime('%Y-%m-%d', datetime / 1000, 'unixepoch')  = '2020-08-02'\n",
    "    GROUP BY strftime('%Y-%m-%d %H:00:00', datetime / 1000, 'unixepoch') ,user\n",
    "        '''\n",
    "cur.execute(query)\n",
    "results = cur.fetchall()\n",
    "print(results)\n",
    "\n",
    "cur.executemany('INSERT INTO Agg_User_Per_Hour VALUES (?,?,?,?);',results)\n",
    "# Commit your changes in the database\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Fetch the aggregated data from Agg_User_Per_Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2020-08-02 07:00:00', 'user_0', 15, 302.1), ('2020-08-02 07:00:00', 'user_1', 13, 10.83), ('2020-08-02 07:00:00', 'user_10', 10, 244.89999999999998), ('2020-08-02 07:00:00', 'user_11', 11, 42.51), ('2020-08-02 07:00:00', 'user_12', 9, 84.87), ('2020-08-02 07:00:00', 'user_13', 12, 61.67), ('2020-08-02 07:00:00', 'user_14', 9, 167.68), ('2020-08-02 07:00:00', 'user_15', 10, 498.26000000000005), ('2020-08-02 07:00:00', 'user_16', 12, 339.04), ('2020-08-02 07:00:00', 'user_17', 8, 22.05), ('2020-08-02 07:00:00', 'user_18', 8, 751.44), ('2020-08-02 07:00:00', 'user_19', 11, 135.75), ('2020-08-02 07:00:00', 'user_2', 9, 174.4), ('2020-08-02 07:00:00', 'user_20', 13, 264), ('2020-08-02 07:00:00', 'user_21', 9, 61.63), ('2020-08-02 07:00:00', 'user_22', 9, 29.73), ('2020-08-02 07:00:00', 'user_23', 12, 686.08), ('2020-08-02 07:00:00', 'user_24', 15, 407.65), ('2020-08-02 07:00:00', 'user_25', 10, 585.4), ('2020-08-02 07:00:00', 'user_26', 13, 674.56), ('2020-08-02 07:00:00', 'user_27', 8, 181.35), ('2020-08-02 07:00:00', 'user_28', 7, 52), ('2020-08-02 07:00:00', 'user_29', 10, 75.24), ('2020-08-02 07:00:00', 'user_3', 10, 873.36), ('2020-08-02 07:00:00', 'user_30', 7, 279.02), ('2020-08-02 07:00:00', 'user_31', 11, 55.62), ('2020-08-02 07:00:00', 'user_32', 4, 754), ('2020-08-02 07:00:00', 'user_33', 17, 628.92), ('2020-08-02 07:00:00', 'user_34', 18, 223.84), ('2020-08-02 07:00:00', 'user_35', 14, 6.65), ('2020-08-02 07:00:00', 'user_36', 11, 257.46), ('2020-08-02 07:00:00', 'user_37', 10, 250.52), ('2020-08-02 07:00:00', 'user_38', 11, 390.32), ('2020-08-02 07:00:00', 'user_39', 9, 269.4), ('2020-08-02 07:00:00', 'user_4', 16, 317.55), ('2020-08-02 07:00:00', 'user_40', 6, 500.64), ('2020-08-02 07:00:00', 'user_41', 11, 119.72), ('2020-08-02 07:00:00', 'user_42', 8, 16.56), ('2020-08-02 07:00:00', 'user_43', 10, 43.37), ('2020-08-02 07:00:00', 'user_44', 14, 79), ('2020-08-02 07:00:00', 'user_45', 11, 136.32), ('2020-08-02 07:00:00', 'user_46', 7, 51.849999999999994), ('2020-08-02 07:00:00', 'user_47', 7, 63.87), ('2020-08-02 07:00:00', 'user_48', 6, 185.64), ('2020-08-02 07:00:00', 'user_49', 11, 697.9499999999999), ('2020-08-02 07:00:00', 'user_5', 8, 383.52), ('2020-08-02 07:00:00', 'user_50', 12, 280.49), ('2020-08-02 07:00:00', 'user_51', 11, 105.56), ('2020-08-02 07:00:00', 'user_52', 10, 20.88), ('2020-08-02 07:00:00', 'user_53', 9, 1.7999999999999998)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"SELECT * FROM Agg_User_Per_Hour\")\n",
    "results = cur.fetchall()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible Flaws in this workflow:\n",
    "1. We have encorporated both insertion and selection logic in the same script. Hence, if we wish to see the same data multiple times, then there is a probability that duplicate records may get insert into the table every time. To avoid this, we can split these scripts into two and separate the insertion and selection logic. It will reduce the operation time and avoid duplicity.\n",
    "2. Further, if we want to prevent same file to get insert multiple times in the database, then we can store the filename and it's upload date-time in separate column and put validation on it.\n",
    "3. We can utilize available data in more detailed format and generate various reports like:\n",
    "- Total revenue by each operation\n",
    "- No.of transactions per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "We can analyse this data using various tools like node JS, Python, Tableau, Power BI etc. Additionally, We can visualize this data in Tableau or Power BI to make it easy to understand and presentale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
